{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aec77fb",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 1\n",
    "#### Student Name: Jiawei Ren\n",
    "#### Student ID: 32073119\n",
    "\n",
    "## Task 2: Parsing Text Files\n",
    "<br> 1. Generate the corpus vocabulary\n",
    "<br> 2. generate the sparse representation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def322a6",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [1. Import Library](#task1)\n",
    "* [2. read excel files and stopwords from current working directory](#task2)\n",
    "* [3. Read data into one dataframe and change the format of reviewTime](#task3)\n",
    "* [4. Tokenizationï¼Œ case normalization and removing stop words for string in reviewText and summary](#task4)\n",
    "* [5. Count word frequency and find rare words](#task5)\n",
    "* [6. Find context dependent stopwords](#task6)\n",
    "* [7. Apply Porterstemmer to words in dataframe](#task7)\n",
    "* [8. Combine unigrams and bigrams,then sort alphabetically](#task8)\n",
    "* [9. create countvectorizer](#task9)\n",
    "\n",
    "## Step 1 Import Library: <a class=\"anchor\" id=\"task1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72e74bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import pandas as pd # for manage excel files and dataframe\n",
    "import os  #for removing files\n",
    "from nltk.corpus import stopwords #for removing stop words and create vocabulary\n",
    "from nltk.tokenize import RegexpTokenizer # for creating tokenizer and tokens\n",
    "from nltk.stem import PorterStemmer #for performing porterstemmer\n",
    "from nltk.probability import * #count frequency of word\n",
    "from sklearn.feature_extraction.text import CountVectorizer #for creating count vectorizer \n",
    "from nltk.util import ngrams #for creating n-grams\n",
    "from nltk.tokenize import MWETokenizer # for multi-word expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98afff6a",
   "metadata": {},
   "source": [
    "## Step 2 read excel files and stopwords from current working directory <a class=\"anchor\" id=\"task2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e57a59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read unstructured EXCEL files\n",
    "df1 = pd.read_excel(\"32073119.xlsx\", sheet_name = 0, header = 0, usecols = \"J:M\") \n",
    "df2 = pd.read_excel(\"32073119.xlsx\", sheet_name = 1, header = 7, usecols = \"B:D\") \n",
    "df3 = pd.read_excel(\"32073119.xlsx\", sheet_name = 2, header = 9, usecols = \"G:I\") \n",
    "df4 = pd.read_excel(\"32073119.xlsx\", sheet_name = 3, header = 6, usecols = \"A:C\") \n",
    "df5 = pd.read_excel(\"32073119.xlsx\", sheet_name = 4, header = 7, usecols = \"B:D\") \n",
    "df6 = pd.read_excel(\"32073119.xlsx\", sheet_name = 5, header = 1, usecols = \"I:K\") \n",
    "df7 = pd.read_excel(\"32073119.xlsx\", sheet_name = 6, header = 2, usecols = \"C:E\") \n",
    "df8 = pd.read_excel(\"32073119.xlsx\", sheet_name = 7, header = 9, usecols = \"E:G\") \n",
    "df9 = pd.read_excel(\"32073119.xlsx\", sheet_name = 8, header = 4, usecols = \"E:G\") \n",
    "df10 = pd.read_excel(\"32073119.xlsx\", sheet_name = 9, header = 7, usecols = \"D:F\") \n",
    "df11 = pd.read_excel(\"32073119.xlsx\", sheet_name = 10, header = 2, usecols = \"D:F\") \n",
    "df12 = pd.read_excel(\"32073119.xlsx\", sheet_name = 11, header = 0, usecols = \"C:E\") \n",
    "df13 = pd.read_excel(\"32073119.xlsx\", sheet_name = 12, header = 9, usecols = \"G:I\") \n",
    "df14 = pd.read_excel(\"32073119.xlsx\", sheet_name = 13, header = 5, usecols = \"J:L\") \n",
    "df15 = pd.read_excel(\"32073119.xlsx\", sheet_name = 14, header = 9, usecols = \"D:F\") \n",
    "df16 = pd.read_excel(\"32073119.xlsx\", sheet_name = 15, header = 4, usecols = \"C:E\") \n",
    "df17 = pd.read_excel(\"32073119.xlsx\", sheet_name = 16, header = 9, usecols = \"H:J\") \n",
    "df18 = pd.read_excel(\"32073119.xlsx\", sheet_name = 17, header = 6, usecols = \"F:H\") \n",
    "df19 = pd.read_excel(\"32073119.xlsx\", sheet_name = 18, header = 7, usecols = \"D:F\") \n",
    "df20 = pd.read_excel(\"32073119.xlsx\", sheet_name = 19, header = 4, usecols = \"H:J\") \n",
    "#read stopwords\n",
    "stopwords = []\n",
    "with open('stopwords_en.txt') as f:\n",
    "    for word in f:\n",
    "        stopwords.append(word.strip())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b55389",
   "metadata": {},
   "source": [
    "## Step 3 Read data into one dataframe and change the format of reviewTime <a class=\"anchor\" id=\"task3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1db3bedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full dataframe \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This down-to-earth, matter-of-fact book will either bring to remembrance the rules of punctuation you once knew, or, if you never learned them, there's enough information to learn a whole new set of skills. It's not only helpful, but also entertaining. I enjoyed reading it, and found it to be helpful.</td>\n",
       "      <td>Outstanding</td>\n",
       "      <td>08/03/2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Man I need part 5 like right now.. I love your pen game your a beast at what you do. I need Rylan and Iman to become one 4real</td>\n",
       "      <td>LUVED IT !!!!!</td>\n",
       "      <td>24/06/2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My grandson had filled this in within two days of receipt. It was fun, and he loved it, but I do with it had had more pages. He loves dot-to-dot!</td>\n",
       "      <td>This Didn't Last Long</td>\n",
       "      <td>15/10/2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Her Place in Time is my 500th Kindle book since I began reading e-books in 2010. Her Place in Time is one of the rare books I would rate at 6+ Stars! Yes, it's that good.However, in order to enjoy this book, you need to be a reader that appreciates a blend of high-energy action adventure spiced with some steamy romance.  I adore well-written time travel stories and this book exceeded my expectations.  In fact, Ms. Jackson does such exceptional world building in her novel that I felt I was actually seeing a movie with great dialogue! Sights, sounds, smells were so wonderfully portrayed that I felt I was actually in the middle of all the action. When the h swung her blade, I swung a blade and when she fought the assassins, I was fighting right along side with her....Yes, this book is for all of us who can cheer a smart, strong self-sufficient female warrior handling herself in any situation (while still maintaining her humanity) and flinch at the typical simpering, whimpering, helpless, sweet female heroines.I was hooked from the beginning of the book to the very last page. I won't give you any spoilers but the story is about a highly trained black ops agent who after an emotionally devastating mission goes into hiding from her own agency who have sent assassins to eliminate her.  She finds a way to go back in time to the early Viking period where her warrior skills are highly valued.The Kindle price is a bit more than I normally pay but it was so worth it.  I also purchased Ms. Jackson's other time travel, Timeless Knights, which I equally devoured with pleasure!I truly hope in the near future that Ms. Jackson gives David his own time travel novel.  Ms. Jackson, please focus your energies in this genre as there are so few well written time travel novels available.</td>\n",
       "      <td>Uber Strong Heroine</td>\n",
       "      <td>13/01/2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>While not my favorite of the trilogy, this waw tied with the second installment. Both characters were lovable and you wanted them to win from the moment they began working as pairs against all odds.</td>\n",
       "      <td>Pair Skating with a flair</td>\n",
       "      <td>20/07/2008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          reviewText  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     This down-to-earth, matter-of-fact book will either bring to remembrance the rules of punctuation you once knew, or, if you never learned them, there's enough information to learn a whole new set of skills. It's not only helpful, but also entertaining. I enjoyed reading it, and found it to be helpful.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Man I need part 5 like right now.. I love your pen game your a beast at what you do. I need Rylan and Iman to become one 4real   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  My grandson had filled this in within two days of receipt. It was fun, and he loved it, but I do with it had had more pages. He loves dot-to-dot!   \n",
       "3  Her Place in Time is my 500th Kindle book since I began reading e-books in 2010. Her Place in Time is one of the rare books I would rate at 6+ Stars! Yes, it's that good.However, in order to enjoy this book, you need to be a reader that appreciates a blend of high-energy action adventure spiced with some steamy romance.  I adore well-written time travel stories and this book exceeded my expectations.  In fact, Ms. Jackson does such exceptional world building in her novel that I felt I was actually seeing a movie with great dialogue! Sights, sounds, smells were so wonderfully portrayed that I felt I was actually in the middle of all the action. When the h swung her blade, I swung a blade and when she fought the assassins, I was fighting right along side with her....Yes, this book is for all of us who can cheer a smart, strong self-sufficient female warrior handling herself in any situation (while still maintaining her humanity) and flinch at the typical simpering, whimpering, helpless, sweet female heroines.I was hooked from the beginning of the book to the very last page. I won't give you any spoilers but the story is about a highly trained black ops agent who after an emotionally devastating mission goes into hiding from her own agency who have sent assassins to eliminate her.  She finds a way to go back in time to the early Viking period where her warrior skills are highly valued.The Kindle price is a bit more than I normally pay but it was so worth it.  I also purchased Ms. Jackson's other time travel, Timeless Knights, which I equally devoured with pleasure!I truly hope in the near future that Ms. Jackson gives David his own time travel novel.  Ms. Jackson, please focus your energies in this genre as there are so few well written time travel novels available.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             While not my favorite of the trilogy, this waw tied with the second installment. Both characters were lovable and you wanted them to win from the moment they began working as pairs against all odds.   \n",
       "\n",
       "                     summary  reviewTime  \n",
       "0                Outstanding  08/03/2014  \n",
       "1             LUVED IT !!!!!  24/06/2014  \n",
       "2      This Didn't Last Long  15/10/2013  \n",
       "3        Uber Strong Heroine  13/01/2013  \n",
       "4  Pair Skating with a flair  20/07/2008  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set the length of column, so I can see the full text\n",
    "pd.options.display.max_colwidth = 100000\n",
    "#make a concat list\n",
    "df_list = [df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,df17,df18,df19,df20]\n",
    "appended_data = pd.concat(df_list)\n",
    "# create a combined EXCEL file and read it \n",
    "appended_data.to_excel('appended.xlsx')\n",
    "df = pd.read_excel('appended.xlsx', header = 0, usecols = \"B:D\") \n",
    "#convert reviewTime to datetime format\n",
    "df['reviewTime'] = pd.to_datetime(df['reviewTime'])\n",
    "df['reviewTime'] = df['reviewTime'].dt.strftime('%d/%m/%Y')\n",
    "print(\"full dataframe \\n\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacb0100",
   "metadata": {},
   "source": [
    "## Step 4 Tokenizationï¼Œ case normalization and removing stop words for string in reviewText and summary  <a class=\"anchor\" id=\"task4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66386a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized dataframe \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[down-to, earth, matter-of, fact, book, bring, remembrance, rules, punctuation, knew, learned, information, learn, set, skills, helpful, entertaining, enjoyed, reading, found, helpful]</td>\n",
       "      <td>[outstanding]</td>\n",
       "      <td>08/03/2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[man, part, love, pen, game, beast, rylan, iman, real]</td>\n",
       "      <td>[luved]</td>\n",
       "      <td>24/06/2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[grandson, filled, days, receipt, fun, loved, pages, loves, dot-to, dot]</td>\n",
       "      <td>[long]</td>\n",
       "      <td>15/10/2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[place, time, kindle, book, began, reading, e-books, place, time, rare, books, rate, stars, good, order, enjoy, book, reader, appreciates, blend, high-energy, action, adventure, spiced, steamy, romance, adore, well-written, time, travel, stories, book, exceeded, expectations, fact, jackson, exceptional, world, building, felt, movie, great, dialogue, sights, sounds, smells, wonderfully, portrayed, felt, middle, action, swung, blade, swung, blade, fought, assassins, fighting, side, book, cheer, smart, strong, self-sufficient, female, warrior, handling, situation, maintaining, humanity, flinch, typical, simpering, whimpering, helpless, sweet, female, heroines, hooked, beginning, book, page, give, spoilers, story, highly, trained, black, ops, agent, emotionally, devastating, mission, hiding, agency, assassins, eliminate, finds, back, time, ...]</td>\n",
       "      <td>[uber, strong, heroine]</td>\n",
       "      <td>13/01/2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[favorite, trilogy, waw, tied, installment, characters, lovable, wanted, win, moment, began, working, pairs, odds]</td>\n",
       "      <td>[pair, skating, flair]</td>\n",
       "      <td>20/07/2008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             reviewText  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              [down-to, earth, matter-of, fact, book, bring, remembrance, rules, punctuation, knew, learned, information, learn, set, skills, helpful, entertaining, enjoyed, reading, found, helpful]   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [man, part, love, pen, game, beast, rylan, iman, real]   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              [grandson, filled, days, receipt, fun, loved, pages, loves, dot-to, dot]   \n",
       "3  [place, time, kindle, book, began, reading, e-books, place, time, rare, books, rate, stars, good, order, enjoy, book, reader, appreciates, blend, high-energy, action, adventure, spiced, steamy, romance, adore, well-written, time, travel, stories, book, exceeded, expectations, fact, jackson, exceptional, world, building, felt, movie, great, dialogue, sights, sounds, smells, wonderfully, portrayed, felt, middle, action, swung, blade, swung, blade, fought, assassins, fighting, side, book, cheer, smart, strong, self-sufficient, female, warrior, handling, situation, maintaining, humanity, flinch, typical, simpering, whimpering, helpless, sweet, female, heroines, hooked, beginning, book, page, give, spoilers, story, highly, trained, black, ops, agent, emotionally, devastating, mission, hiding, agency, assassins, eliminate, finds, back, time, ...]   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [favorite, trilogy, waw, tied, installment, characters, lovable, wanted, win, moment, began, working, pairs, odds]   \n",
       "\n",
       "                   summary  reviewTime  \n",
       "0            [outstanding]  08/03/2014  \n",
       "1                  [luved]  24/06/2014  \n",
       "2                   [long]  15/10/2013  \n",
       "3  [uber, strong, heroine]  13/01/2013  \n",
       "4   [pair, skating, flair]  20/07/2008  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create tokenizer pattern\n",
    "regexptokenizer = RegexpTokenizer(\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "dftoken = df\n",
    "vocab = []\n",
    "text_vocab = []\n",
    "for i in range(10000):\n",
    "    #tokenization\n",
    "    filtered_reviewText = []\n",
    "    filtered_summary = []\n",
    "    words1 = regexptokenizer.tokenize(str(dftoken[\"reviewText\"][i]))\n",
    "    words2 = regexptokenizer.tokenize(str(dftoken[\"summary\"][i]))\n",
    "    #case normalization\n",
    "    for x in range(len(words1)):\n",
    "        words1[x] = words1[x].lower()\n",
    "    for x in range(len(words2)):\n",
    "        words2[x] = words2[x].lower()\n",
    "    #remove context-independent stopwords and token with a length less than 3\n",
    "    for word in words1:\n",
    "        if word not in stopwords and len(word) >= 3:\n",
    "            filtered_reviewText.append(word)\n",
    "    for word in words2:\n",
    "        if word not in stopwords and len(word) >= 3:  \n",
    "            filtered_summary.append(word)\n",
    "    #replace string with token list\n",
    "    dftoken.at[i, 'reviewText'] = filtered_reviewText\n",
    "    dftoken.at[i, 'summary'] = filtered_summary\n",
    "    #store all tokens in a list\n",
    "    vocab = vocab + filtered_summary + filtered_reviewText\n",
    "#use set to get a unique vocabulary\n",
    "vocab = set(vocab)\n",
    "print(\"tokenized dataframe \\n\")\n",
    "dftoken.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7079f49",
   "metadata": {},
   "source": [
    "## Step 5 Count word frequency and find rare words  <a class=\"anchor\" id=\"task5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7ab1bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of rare word \n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_date = []\n",
    "date_word= []\n",
    "word_count ={}\n",
    "#create a list of unique date\n",
    "for date in dftoken[\"reviewTime\"]:\n",
    "    if date not in unique_date:\n",
    "        unique_date.append(date)\n",
    "# count word by date\n",
    "for date in unique_date:\n",
    "    dword = []\n",
    "    for texts in dftoken[\"reviewText\"].loc[dftoken['reviewTime'] == date]:\n",
    "        for text in texts:\n",
    "            dword.append(text)\n",
    "            #record each word in the dictionary\n",
    "            word_count[text] = 0\n",
    "    for summary in dftoken[\"summary\"].loc[dftoken['reviewTime'] == date]:\n",
    "        for text in summary:\n",
    "            dword.append(text)\n",
    "            word_count[text] = 0\n",
    "    # this list is like [a date, and all unique tokens at this day]\n",
    "    date_word.append([date,set(dword)])\n",
    "    \n",
    "for i in range(len(date_word)):\n",
    "    # count the frequency of words\n",
    "    for word in date_word[i][1]:\n",
    "        word_count[word] +=1\n",
    "rare_word = []\n",
    "print(\"list of rare word \\n\")\n",
    "for (key, value) in word_count.items():\n",
    "    if value <10:\n",
    "        rare_word.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39e8ba55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I found 40130 rare words\n"
     ]
    }
   ],
   "source": [
    "print(\"I found {} rare words\".format(len(rare_word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4d68136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rare word from vocabulary and dictionary\n",
    "for word in rare_word:\n",
    "    vocab.remove(word)\n",
    "    word_count.pop(word, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8217fc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now I have 6270 words in vocabulary\n"
     ]
    }
   ],
   "source": [
    "print(\"Now I have {} words in vocabulary\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc21b61b",
   "metadata": {},
   "source": [
    "## Step 6 Find context dependent stopwords  <a class=\"anchor\" id=\"task6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b89e67fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'natalie', 'woody', 'communist', 'jesse', 'taboo', 'emma', 'adam', 'antennas', 'cia', 'cooker', 'arthur', 'brandon', 'aaron', 'stones', 'restoration', 'kong', 'linux', 'raven', 'physics', 'finder', 'roku', 'networking', 'iso', 'hannah', 'josh', 'walter', 'compartment', 'carol', 'musicals', 'pearl', 'sophie', 'leap', 'cole', 'theories', 'camcorder', 'cheese', 'savage', 'todd', 'spring', 'batman', 'jeans', 'ally', 'india', 'collins', 'lucy', 'hardy', 'kristen', 'claire', 'maggie', 'norman', 'texture', 'tommy', 'letters', 'guest', 'philosophical', 'bose', 'seattle', 'pioneer', 'superman', 'liam', 'lex', 'sata', 'reed', 'quantum', 'holocaust', 'harper', 'wwe', 'earphones', 'clay', 'kyle', 'ross', 'painting', 'dick', 'seth', 'dylan', 'cloud', 'raid', 'trips', 'lily', 'bout', 'walt', 'poems', 'nathan', 'boom', 'matrix', 'ellen', 'meal', 'moore', 'andrew', 'converter', 'bailey', 'scripture', 'cruise', 'bandwidth', 'ash', 'criterion', 'sally', 'ring', 'mars', 'maximum', 'june', 'matthew', 'van', 'antenna', 'bobby', 'destiny', 'ninja', 'flag', 'championship'}\n",
      "\n",
      "\n",
      "number of context_dependent words found: 109\n"
     ]
    }
   ],
   "source": [
    "context_dependent = []\n",
    "for row in range(10000):\n",
    "    # count the frequency of words in a cell\n",
    "    f_summary = FreqDist(dftoken[\"summary\"][row])\n",
    "    f_review = FreqDist(dftoken[\"reviewText\"][row])\n",
    "    for text in dftoken[\"reviewText\"][row]:\n",
    "        # pass if the frequency is greater than Number_of_days / 2\n",
    "        if text in word_count and f_review[text] > word_count[text]/2:\n",
    "            context_dependent.append(text)\n",
    "    for text in dftoken[\"summary\"][row]:\n",
    "        if text in word_count and f_summary[text] > word_count[text]/2:\n",
    "            context_dependent.append(text)\n",
    "#get a unique list of context dependent words\n",
    "context_dependent = set(context_dependent)\n",
    "print(context_dependent)\n",
    "print(\"\\n\\nnumber of context_dependent words found:\", len(context_dependent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "115e8e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove context dependent words from vocabulary and dictionary \n",
    "for word in context_dependent:\n",
    "    vocab.remove(word)\n",
    "    word_count.pop(word, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76d15966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique words now: 6161\n"
     ]
    }
   ],
   "source": [
    "print(\"number of unique words now:\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268bad07",
   "metadata": {},
   "source": [
    "## Step 7 Apply Porterstemmer to words in dataframe  <a class=\"anchor\" id=\"task7\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91933ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pstemmer = PorterStemmer()\n",
    "all_tokens = []\n",
    "date_token = []\n",
    "#Apply Porterstemmer to words in dataframe\n",
    "for row in range(10000):\n",
    "    filtered_reviewText = []\n",
    "    filtered_summary = []\n",
    "    for text in dftoken[\"reviewText\"][row]:\n",
    "        if text in vocab:\n",
    "            filtered_reviewText.append(pstemmer.stem(text))\n",
    "    for text in dftoken[\"summary\"][row]:\n",
    "        if text in vocab:\n",
    "            filtered_summary.append(pstemmer.stem(text))\n",
    "    dftoken.at[row, 'reviewText'] = filtered_reviewText\n",
    "    dftoken.at[row, 'summary'] = filtered_summary\n",
    "    all_tokens = all_tokens + filtered_reviewText + filtered_summary\n",
    "    date_token.append([dftoken[\"reviewTime\"][row],filtered_reviewText + filtered_summary])\n",
    "uni_voc = list(set(all_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dde9327",
   "metadata": {},
   "source": [
    "Find top 200 meaningful bigrams by using PMI measure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12f7772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(all_tokens)\n",
    "best_bigram = finder.nbest(bigram_measures.pmi, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f91a9e",
   "metadata": {},
   "source": [
    "## Step 8 Combine unigrams and bigrams,then sort alphabetically  <a class=\"anchor\" id=\"task8\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "813a65d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine and convert to multi-word expressions.\n",
    "for bigram in best_bigram:\n",
    "    uni_voc.append(bigram)\n",
    "\n",
    "mwe_tokenizer = MWETokenizer(uni_voc)\n",
    "mwe_tokens = mwe_tokenizer.tokenize(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdfa98d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort alphabetically\n",
    "mwe_tokens = list(set(mwe_tokens))\n",
    "mwe_tokens = sorted(mwe_tokens, key=str.lower)\n",
    "# give ID number\n",
    "vocab_id = {}\n",
    "for i in range(len(mwe_tokens)):\n",
    "    vocab_id[mwe_tokens[i]] = i\n",
    "\n",
    "with open(\"32073119_vocab.txt\",\"a\") as f:\n",
    "    for word, count in vocab_id.items():\n",
    "        f.write(\"{words}:{id}\\n\".format(words=word,id=count ))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3fc5029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregate texts by date\n",
    "date_dictionary = {}\n",
    "for date in unique_date:\n",
    "    date_dictionary[date] = ''\n",
    "\n",
    "for i in range(len(date_token)):\n",
    "    string= ' '.join([str(item) for item in date_token[i][1]])\n",
    "    date_dictionary[date_token[i][0]] = date_dictionary[date_token[i][0]] + string + ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b7adf6",
   "metadata": {},
   "source": [
    "## Step 9 create countvectorizer  <a class=\"anchor\" id=\"task9\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1139cd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = {}\n",
    "#create countvectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\")\n",
    "for date in unique_date:\n",
    "    data_features[date] = vectorizer.fit_transform([date_dictionary[date]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1067a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2 = vectorizer.get_feature_names()\n",
    "with open(\"32073119_countVec.txt\",\"a\") as f:\n",
    "    for i in range(len(unique_date)):\n",
    "        message = '{}'.format(unique_date[i])\n",
    "        for word, count in zip(mwe_tokens, data_features[unique_date[i]].toarray()[0]):\n",
    "            if count > 0:\n",
    "                w = \",{words}:{count}\".format(words=vocab_id[word],count=count )\n",
    "                message = message + w\n",
    "        f.write(message)\n",
    "        f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dbaed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "7758e92e9a61d7a3490898707f7eeb937c85e9d1e8d4e877cc6c187218f226d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
